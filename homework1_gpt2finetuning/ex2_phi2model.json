{"model_name": "Microsoft Phi-2", "huggingface_model_id": "microsoft/phi-2", "paper_url": "https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/", "tokenizer_type": "BPE (CodeGen Tokenizer)", "vocabulary_size": "51200", "architecture": "transformer stack with causal language modeling head", "architecture_type": "decoder only", "architecture_quirks": ["vocab size discrepancy - it has embedding size as 51200 to accommodate any new tokens", "can choose/configure among three attention classes - regular attention, flash attention, and scaled dot product attention", "has attention overflow issue with FP16, requires enabling/disabling autocast on PhiAttention.forward() function"], "parameters": "2.7B", "finetuning_type": "not finetuned", "training_data_cutoff": "2023", "number_training_tokens": "1.4T tokens", "pretraining_data_size": "250B tokens", "finetuning_data_size": "NA", "training_data": ["python codes from The Stack v1.2", "q&a content from StackOverflow", "google-deepmind code_contests dataset", "synthetic python textbooks and exercises generated by gpt-3.5-turbo-0301", "various NLP synthetic texts", "filtered websites (Falcon RefinedWeb and SlimPajama assessed by gpt-4) for safety and educational value"], "finetuning_data": [], "access": "open", "summary": "Phi-2 is a relatively smaller language model (only 2.7B params), but it surpasses or matches performances of models upto 25x larger (Mistral (7B) or Llama-2 (13B) models) on coding and math tasks. It was neither further finetuned with RLHF, nor instruction fine-tuned, so its generation capability is non-restricted, and because of its compact size, researchers can use it to explore safety, fairness and bias related challenges in LMs."}